{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_community.tools import BraveSearch\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "import getpass\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from cachetools import TTLCache\n",
    "from langchain import hub\n",
    "from langchain.utilities import WikipediaAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "set_llm_cache(InMemoryCache())  # Initialize the cache\n",
    "qa_cache=TTLCache(maxsize=100,ttl=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", region_name=\"us-east-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store=AstraDBVectorStore(\n",
    "    collection_name=\"trail\",\n",
    "    embedding=embeddings,\n",
    "    token= os.getenv(\"token\"),\n",
    "    api_endpoint=os.getenv(\"api_end_point\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://blog.langchain.dev/memory-for-agents/\")\n",
    "document=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "doc_split=text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ebeb5021f5ce435f8ee56b8cb3c94b34',\n",
       " '3e2f5871d5694df9bd6b3fea93e2fea7',\n",
       " '21100be27a464707b1ad8e75398afe2b',\n",
       " 'd2595cd006b34ebbadd23fbc2417b7f3',\n",
       " '9bd086cf48884b898ee723f49689b1bd',\n",
       " '38130aecce3a494d81f32caa7daae152',\n",
       " 'f4eb6bdc22354393a8f8f428df6ba7a9',\n",
       " '39597dd20fe149308ab7ee9653a372c9',\n",
       " '7d28030a0eda4dc48c47ac8117132804',\n",
       " 'a46e8302f8f94561a53bacdb346aa829',\n",
       " '07bee337eb3344d09297f2f1de6f0937']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectore_store.add_documents(doc_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "global result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=vectore_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":1},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=result.invoke(\"what is memory in agents?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='012d20a159b54bfea5d89701c77e2d12', metadata={'source': 'https://blog.langchain.dev/memory-for-agents/', 'title': 'Memory for agents', 'language': 'en'}, page_content='memory in agents: the CoALA paper defines episodic memory as storing sequences of the agent’s past actions.This is used primarily to get an agent to perform as intended.In practice, episodic memory is implemented as few-shot example prompting. If you collect enough of these sequences, then this can be done via dynamic few-shot prompting. This is usually great for guiding the agent if there is a correct way to perform specific actions that have been done before. In contrast, semantic memory is more relevant if there isn’t necessarily a correct way to do things, or if the agent is constantly doing new things so the previous examples don’t help much.How to update memoryBesides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory.One way to update agent memory is “in the hot path”. This is where the agent system explicitly decides to remember facts (usually via tool calling) before responding. This is the')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory in agents: the CoALA paper defines episodic memory as storing sequences of the agent’s past actions.This is used primarily to get an agent to perform as intended.In practice, episodic memory is implemented as few-shot example prompting. If you collect enough of these sequences, then this can be done via dynamic few-shot prompting. This is usually great for guiding the agent if there is a correct way to perform specific actions that have been done before. In contrast, semantic memory is more relevant if there isn’t necessarily a correct way to do things, or if the agent is constantly doing new things so the previous examples don’t help much.How to update memoryBesides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory.One way to update agent memory is “in the hot path”. This is where the agent system explicitly decides to remember facts (usually via tool calling) before responding. This is the\n"
     ]
    }
   ],
   "source": [
    "data=\"\"\n",
    "for doc in x:\n",
    "    data+= doc.page_content\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMessageHistory(session_id=\"test-session\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question:str)->str:\n",
    "    if question in qa_cache:\n",
    "        return qa_cache[question]\n",
    "    else:\n",
    "        result=result.invoke(question)\n",
    "        data=\"\"\n",
    "        for doc in x:\n",
    "            data+= doc.page_content\n",
    "        qa_cache[question]=data\n",
    "        return data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_tool=Tool(\n",
    "    name=\"knowldege_base_qa_using_vector\",\n",
    "    func=question_answer,\n",
    "    description=\" answer question based on the knowledge using the vectore store and cache memeory\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia=WikipediaAPIWrapper()\n",
    "w=wikipedia.run(\"langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Page: LangChain\\nSummary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\nPage: Retrieval-augmented generation\\nSummary: Retrieval augmented generation (RAG) is a technique that grants generative artificial intelligence models information retrieval capabilities. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to augment information drawn from its own vast, static training data. This allows LLMs to use domain-specific and/or updated information.  \\nUse cases include providing chatbot access to internal company data, or giving factual information only from an authoritative source.\\n\\n\\n\\nPage: DataStax\\nSummary: DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki(query:str)->str:\n",
    "    wiki_data=wikipedia.run(query)\n",
    "    return wiki_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tool=Tool(\n",
    "    name=\"wikipedia\",\n",
    "    func=wiki,\n",
    "    description=\"search for the information on wikipedia\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki_tool,qa_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=hub.pull(\"hwchase17/react\")\n",
    "agent=create_react_agent(llm,tools,prompt)\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools)\n",
    "agent_with_memory=RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    lambda session_id: memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is memory in agents',\n",
       " 'chat_history': [],\n",
       " 'output': 'In the context of artificial intelligence, \"memory\" refers to the ability of agents to store and retrieve information. Specifically, episodic memory is a type of memory that refers to the memory of experiences and specific events that occur in an agent\\'s \"life\". This is distinct from semantic memory, which refers to general world knowledge that an agent has accumulated throughout its \"life\".'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent_with_memory.invoke(\n",
    "    {\"input\":\"what is memory in agents\"},\n",
    "    config={\"configurable\":{\"session_id\":\"session\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachetools import TTLCache\n",
    "set_llm_cache=TTLCache(maxsize=100,ttl=600)\n",
    "def llm_with_cache(inputs):\n",
    "    if inputs in set_llm_cache:\n",
    "        return set_llm_cache[inputs]\n",
    "    else:\n",
    "        z=agent_with_memory.invoke(\n",
    "    {\"input\":inputs},\n",
    "    config={\"configurable\":{\"session_id\":\"session\"}}\n",
    ")\n",
    "        set_llm_cache[inputs]=z\n",
    "        return z\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=llm_with_cache(\"what is memory in agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'who is amir khan?',\n",
       " 'output': 'Amir Khan can refer to two different individuals: Amir Iqbal Khan, a British former professional boxer, or Aamir Khan, an Indian actor, filmmaker, and television personality. Without more context, it is unclear which Amir Khan is being referred to.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_without_memory=agent_executor\n",
    "agent_without_memory.invoke({\"input\":\"who is amir khan?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context of agents, \"memory\" refers to the ability of an artificial intelligence system to store and retrieve information, similar to human memory. This can include short-term memory, long-term memory, and other forms of memory storage. In artificial intelligence, memory is often implemented using computer memory, such as RAM or flash memory, and can be used to store data, programs, and other information needed for the AI system to function. In-memory processing is a technique used in AI systems to improve performance by storing and processing data in memory rather than on disk storage. Additionally, AI accelerators, such as neural processing units (NPUs), are designed to accelerate AI and machine learning applications, including those that rely on memory storage and retrieval.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
